import requests
import time
from airflow import DAG
from airflow.operators.http_operator import SimpleHttpOperator
from airflow.operators.python_operator import PythonOperator  # Import PythonOperator
from airflow.sensors.http_sensor import HttpSensor
from airflow.sensors.time_delta import TimeDeltaSensor
from airflow.utils.dates import days_ago
from datetime import timedelta
from airflow.providers.slack.notifications.slack import send_slack_notification
import json

{% for dag_ in dag %}
dag = DAG(
    "{{ dag_.dag_id }}",
    schedule_interval="{{ dag_.schedule_interval }}",
    start_date=days_ago(1),
    catchup={{ dag_.catchup or False }},
)
{% endfor %}

{% set delay_durations = data_seconds %}
duration_data = {{ data_seconds }}

{% set http_tasks = [] %}
http_tasks = []
{% set prev_response = None %}

# Define the first HTTP operator separately
http_task_1 = SimpleHttpOperator(
    task_id="{{ operators[0].task_id }}",
    http_conn_id="{{ operators[0].http_conn_id }}",
    endpoint="{{ operators[0].endpoint }}",
    method="{{ operators[0].method }}",
    headers={{ operators[0].headers | tojson }},  # Assuming headers is a dictionary, convert it to JSON
    data=json.dumps({{ operators[0].data}}),
    log_response={{ operators[0].log_response }},
    dag=dag,
    do_xcom_push=True,
    on_failure_callback=[
        send_slack_notification(
            text="The task {{ operators[0].task_id }} failed",
            channel="#airflow-error",
            username="airflow",
        )
    ],
)
http_tasks.append(http_task_1)

# Set up task dependencies with dynamic delay durations
{% if operators|length > 1 %}
{% for i in range(1, operators|length) %}

# Downstream task to retrieve XCom value
{% if i >= 1 %}
def process_json_response_{{i}}(**context):
    ti = context['task_instance']
    prev_json_data = ti.xcom_pull(task_ids='process_response_{{i-1}}_task', key='json_response')
    chunk_size = 1
    if prev_json_data:
        response = None  
        if "data" in prev_json_data:
            response = requests.post(url = "{{operators[i].url}}", data=json.dumps(prev_json_data['data']))
        else:
            response = requests.post(url = "{{operators[i].url}}", data=json.dumps(prev_json_data))
        
        json_data = response.json()
    
        context['task_instance'].xcom_push(key='json_response', value=json_data)
    else:
        prev_json_data = ti.xcom_pull(task_ids='{{ operators[i-1].task_id }}', key='return_value')
        response = None
        print(prev_json_data)
        if "data" in prev_json_data:
            print(json.loads(prev_json_data).get("data", []))
            data_list = json.loads(prev_json_data).get("data", [])  # Get the list of dictionaries under "data" key
            if data_list:
                    for i in range(0, len(data_list), chunk_size):
                        chunk = data_list[i:i + chunk_size]
                        print(chunk)
                        # Send the chunk of requests
                        response = requests.post(url="{{operators[i].url}}", json=chunk)
                    
                        # Check the response status code
                        if response.status_code == 200:
                            print(response.json())
                            json_data = response.json()
                            # context['task_instance'].xcom_push(key='json_response', value=json_data)
                        else:
                            print(f'Request failed with status code {response.status_code}')
                    
                        # Wait for 20 seconds before sending the next chunk
                        time.sleep(7)
            else:
                raise Exception("the dataset is empty")
                
            
        
            
        

process_response_{{i}}_task = PythonOperator(
    task_id='process_response_{{ operators[i].task_id }}',
    python_callable=process_json_response_{{i}},
    provide_context=True,
    retries=3,  # Override the retries for this task
    retry_delay=timedelta(minutes=420), 
    dag=dag
)

# Define the HTTP operator
http_task_{{ i + 1 }} = SimpleHttpOperator(
    task_id="{{ operators[i].task_id }}",
    http_conn_id="{{ operators[i].http_conn_id }}",
    endpoint="{{ operators[i].endpoint }}",
    method="{{ operators[i].method }}",
    headers={{ operators[i].headers | tojson }},  # Assuming headers is a dictionary, convert it to JSON
    data=json.dumps({{ operators[i].data }}),
    log_response={{ operators[i].log_response }},
    do_xcom_push=True,  # Enable pushing response to XCom
    dag=dag,
    on_failure_callback=[
        send_slack_notification(
            text="The task {{ operators[i].task_id }} failed",
            channel="#airflow-error",
            username="airflow",
        )
    ],
)
http_tasks.append(http_task_{{ i + 1 }})

{% endif %}

# Set up task dependencies with dynamic delay durations
{% if loop.index|string in delay_durations and loop.index > 1 and operators[loop.index].data is not none%}
# Add delay sensor between tasks
delay_task_{{ loop.index }} = TimeDeltaSensor(
    task_id='delay_task_{{ loop.index }}',
    delta=timedelta(seconds=duration_data[str({{ loop.index }})]),  # Use dynamic delay durations from the dictionary
    mode='reschedule',
    dag=dag,
)
http_task_{{ loop.index - 1 }} >> delay_task_{{ loop.index }} >> http_task_{{ loop.index }}
{% elif loop.index > 1 and operators[loop.index].data is not none %}
http_task_{{ loop.index - 1 }} >> http_task_{{ loop.index }}
{% elif loop.index|string in delay_durations and loop.index > 1 and operators[loop.index].data is none  %}
process_response_{{loop.index-1}}_task >> delay_task_{{ loop.index }} >> process_response_{{loop.index}}_task 
{% elif loop.index|string not in delay_durations and loop.index > 1 and operators[loop.index].data is none %}
process_response_{{loop.index}}_task 
{% endif %}

{% endfor %}
{% endif %}
